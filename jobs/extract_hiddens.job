#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus=1
#SBATCH --job-name=ExtractHiddens
#SBATCH --ntasks=1
#SBATCH --time=04:00:00
#SBATCH --output=./jobs/output/extract_hiddens_%A.out


module purge
module load 2023
module load Anaconda3/2023.07-2

cd $HOME/thesis/elk-generalization/

source activate elk-generalization

# Vars
model_names=("qm-pythia-410M" "qm-pythia-1B" "qm-pythia-1.4B" "qm-pythia-2.8B" "qm-pythia-6.9B" "qm-pythia-12B" "qm-Llama-2-7b-hf" "qm-Mistral-7B-v0.1")
templates=("grader-first" "grader-last" "mixture")

# Vars for AE->BH
characters=(Alice Bob)
difficulties=(easy hard)
max_examples=(4096 900) # Validating on validation or test only gives <1000 samples, but according to the paper we should test on 1024. Reducing evaluation to 900 to resolve.
splits=(train test)   

for (( m=0; m<${#model_names[@]}; m++ )); do
    model_name=${model_names[m]}

    for (( t=0; t<${#templates[@]}; t++ )); do
        template=${templates[t]}
        for (( i=0; i<${#characters[@]}; i++ )); do
            character=${characters[i]}
            difficulty=${difficulties[i]}
            max_example=${max_examples[i]}
            split=${splits[i]}
            srun python -u elk_generalization/elk/extract_hiddens.py \
                --model EleutherAI/$model_name-$template \
                --dataset EleutherAI/qm-$template \
                --save-path ./experiments/$model_name-$template/$character-$difficulty \
                --character $character \
                --difficulty $difficulty \
                --splits $split \
                --max-examples $max_example
        done
    done
done