{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated knoweldge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from elk_generalization.elk.elk_utils import SplitConfig\n",
    "\n",
    "# Ignore pandas performancewarnings as these df's are small anyways\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "def cfg_sort_key(descriptor):\n",
    "    cfg = SplitConfig.from_descriptor(descriptor)\n",
    "    key1 = cfg.column_to_key[\"objective_labels\"]\n",
    "    key2 = cfg.column_to_key[\"quirky_labels\"]\n",
    "    alignment = cfg.get_alignment(key1=key1, key2=key2)\n",
    "\n",
    "    return str(alignment) + descriptor\n",
    "\n",
    "def alignment_str(descriptor):\n",
    "    cfg = SplitConfig.from_descriptor(descriptor)\n",
    "    key1 = cfg.column_to_key[\"objective_labels\"]\n",
    "    key2 = cfg.column_to_key[\"quirky_labels\"]\n",
    "    alignment = cfg.get_alignment(key1=key1, key2=key2)\n",
    "    alignment_strings = {\n",
    "        -1: \"NEGATIVELY aligned\",\n",
    "        0: \"NOT aligned\",\n",
    "        1: \"POSITIVELY aligned\"\n",
    "    }\n",
    "    return f\"{key1}, {key2} are {alignment_strings[alignment]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options   \n",
    "csv_dir = Path(r\"..\\elk-generalization\\experiments\")\n",
    "csv_filename = Path(\"summary_aligning_20240331_deduplicated.csv\")\n",
    "fig_dir = Path(r\"..\\elk-generalization\\figures\\transfer_align\") / csv_filename.stem\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "heatmap_kwargs = {\"vmin\": -0.5, \"vmax\": 0.5, \"fmt\":'', \"cmap\":'coolwarm', \"cbar\":False}\n",
    "pr_filter_val = False\n",
    "\n",
    "# Constants\n",
    "models = ['pythia-12B', 'pythia-6.9B', 'pythia-2.8B', 'pythia-1.4B', 'pythia-1B', 'pythia-410M']\n",
    "reporters = [\"lr\", \"mean-diff\", \"lda\", \"ccs\", \"crc\", \"lm\"]\n",
    "unsupervised_reporters = [\"ccs\", \"crc\"]\n",
    "supervised_reporters = [\"lr\", \"mean-diff\", \"lda\"]\n",
    "reporters_with_averages = reporters + [\"unsupervised_avg\", \"supervised_avg\"]\n",
    "\n",
    "# Prepare data\n",
    "filtered_df = pd.read_csv(csv_dir / csv_filename, index_col=[0,1,2]) # Indexes are (model, reporter, train_cfg)\n",
    "# df = df.sort_index()\n",
    "# Filter\n",
    "for f in [\"pi=True\", f\"pr={pr_filter_val}\"]:\n",
    "    relevant_configs = filtered_df.index.get_level_values('train_cfg').str.contains(f)\n",
    "    filtered_df = filtered_df.loc[relevant_configs, filtered_df.columns.str.contains(f)]\n",
    "# Add averages\n",
    "for model in models:\n",
    "    for train_cfg in filtered_df.columns:\n",
    "        for test_dataset in filtered_df.columns:\n",
    "            unsupervised_performances = filtered_df.loc[pd.IndexSlice[model, unsupervised_reporters, train_cfg], test_dataset]\n",
    "            supervised_performances = filtered_df.loc[pd.IndexSlice[model, supervised_reporters, train_cfg], test_dataset]\n",
    "            assert len(unsupervised_performances) == len(unsupervised_reporters)\n",
    "            assert len(supervised_performances) == len(supervised_reporters)\n",
    "            \n",
    "            filtered_df.loc[(model, \"unsupervised_avg\", train_cfg), test_dataset] = unsupervised_performances.mean()\n",
    "            filtered_df.loc[(model, \"supervised_avg\", train_cfg), test_dataset] = supervised_performances.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "import os\n",
    "\n",
    "def plot_heatmaps_for_model(df, model, save_dir, **heatmap_kwargs):\n",
    "    reporters = ['lm', 'lr', 'mean-diff', 'lda', 'ccs', 'crc']\n",
    "    titles = ['Baseline (LM output)', 'LR', 'Mean-Diff', 'LDA', 'CCS', 'CRC']\n",
    "   \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "   \n",
    "    # Custom colormap\n",
    "    colors = ['#710000', '#f8696b', '#ffeb84', '#63be7b']  # Dark Red to Red to Yellow to Green\n",
    "    n_bins = 100\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list('custom', colors, N=n_bins)\n",
    "   \n",
    "    for i, (reporter, title) in enumerate(zip(reporters, titles)):\n",
    "        filtered_df = df.loc[model, reporter, :]\n",
    "       \n",
    "        if reporter == 'lm':\n",
    "            # For baseline, use only the first row\n",
    "            filtered_df = filtered_df.iloc[[0]]\n",
    "        else:\n",
    "            # Sort index and columns for other methods\n",
    "            filtered_df = filtered_df.sort_index(key=lambda x: x.map(cfg_sort_key), ascending=False)\n",
    "            sorted_columns = sorted(filtered_df.columns, key=cfg_sort_key, reverse=True)\n",
    "            filtered_df = filtered_df[sorted_columns]\n",
    "       \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(filtered_df, ax=axes[i], cmap=cmap, **heatmap_kwargs)\n",
    "       \n",
    "        axes[i].set_title(title, fontsize=12)\n",
    "        axes[i].set_xticks([])\n",
    "        axes[i].set_yticks([])\n",
    "        \n",
    "        # Add \"Train\" and \"Test\" labels\n",
    "        if i >= 3:  # Only for the bottom row\n",
    "            axes[i].set_xlabel('Test', fontsize=10)\n",
    "        if i % 3 == 0:  # Only for the leftmost column\n",
    "            axes[i].set_ylabel('Train', fontsize=10)\n",
    "   \n",
    "    # Add colorbar\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=1))\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "    cbar.set_label('AUROC', rotation=270, labelpad=15)\n",
    "   \n",
    "    # Add tick for AUROC = 0.5\n",
    "    cbar.ax.axhline(y=0.5, color='k', linestyle='--', linewidth=0.5)\n",
    "    cbar.ax.text(1.5, 0.5, 'Random\\nGuess', va='center', ha='left', fontsize=8)\n",
    "   \n",
    "    plt.suptitle(f'Heatmaps for {model}', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 0.9, 0.95])\n",
    "   \n",
    "    # Save the figure\n",
    "    plt.savefig(os.path.join(save_dir, f'{model}_heatmaps.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "def plot_all_models(df, save_dir):\n",
    "    models = df.index.get_level_values('models').unique()\n",
    "    \n",
    "    heatmap_kwargs = {\n",
    "        \"vmin\": 0, \n",
    "        \"vmax\": 1, \n",
    "        \"cbar\": False, \n",
    "        \"annot\": True, \n",
    "        \"fmt\": '.2f', \n",
    "        \"annot_kws\": {\"size\": 8},\n",
    "        \"linewidths\": 0.5\n",
    "    }\n",
    "    \n",
    "    for model in models:\n",
    "        plot_heatmaps_for_model(df, model, save_dir, **heatmap_kwargs)\n",
    "\n",
    "# Example usage:\n",
    "save_dir = 'figures/transfer_align/appendix'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "plot_all_models(filtered_df, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def plot_3x3_heatmaps_optimized(df, model='pythia-12B', **heatmap_kwargs):\n",
    "    reporters = ['supervised_avg', 'unsupervised_avg', 'lm']\n",
    "    titles = ['Supervised (LR, MM, LDA)', 'Unsupervised (CCS, CRC)', 'Baseline (LM output)']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
    "    \n",
    "    # Custom colormap\n",
    "    colors = ['#710000', '#f8696b', '#ffeb84', '#63be7b']  # Dark Red to Red to Yellow to Green\n",
    "    n_bins = 100\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list('custom', colors, N=n_bins)\n",
    "    \n",
    "    for i, (reporter, title) in enumerate(zip(reporters, titles)):\n",
    "        filtered_df = df.loc[model, reporter, :]\n",
    "        \n",
    "        if reporter == 'lm':\n",
    "            # For baseline, use only the first row (as it's the same for all)\n",
    "            filtered_df = filtered_df.iloc[[0]]\n",
    "            # Reshape to get 3 tall columns\n",
    "            filtered_df = pd.DataFrame(filtered_df.values, columns=['Column 1', 'Column 2', 'Column 3'])\n",
    "        else:\n",
    "            # Sort index and columns for supervised and unsupervised\n",
    "            filtered_df = filtered_df.sort_index(key=lambda x: x.map(cfg_sort_key), ascending=False)\n",
    "            sorted_columns = sorted(filtered_df.columns, key=cfg_sort_key, reverse=True)\n",
    "            filtered_df = filtered_df[sorted_columns]\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(filtered_df, ax=axes[i], cmap=cmap, **heatmap_kwargs)\n",
    "        \n",
    "        axes[i].set_title(title, fontsize=10, wrap=True)\n",
    "        axes[i].set_xticks([])\n",
    "        axes[i].set_yticks([])\n",
    "        axes[i].set_xlabel('')  # Remove x-axis label\n",
    "        axes[i].set_ylabel('')  # Remove y-axis label\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=1))\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "    \n",
    "    # Add AUROC label above the colorbar\n",
    "    cbar.ax.text(0.5, 1.05, 'AUROC', ha='center', va='bottom', transform=cbar.ax.transAxes)\n",
    "    \n",
    "    # Add tick for AUROC = 0.5\n",
    "    cbar.ax.axhline(y=0.5, color='k', linestyle='--', linewidth=0.5)\n",
    "    cbar.ax.text(1.5, 0.5, 'Random\\nGuess', va='center', ha='left', fontsize=8)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=0.9, wspace=0.1)  # Make room for colorbar and reduce space between subplots\n",
    "    \n",
    "    return fig, axes\n",
    "\n",
    "# Example usage:\n",
    "heatmap_kwargs = {\n",
    "    \"vmin\": 0, \n",
    "    \"vmax\": 1, \n",
    "    \"cbar\": False, \n",
    "    \"annot\": True, \n",
    "    \"fmt\": '.2f', \n",
    "    \"annot_kws\": {\"size\": 10},  # Increase font size of numbers\n",
    "    \"linewidths\": 0.5\n",
    "}\n",
    "fig, axes = plot_3x3_heatmaps_optimized(filtered_df, **heatmap_kwargs)\n",
    "plt.savefig('figures/transfer_align/simulated_knowledge_heatmaps.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "temp_df = None\n",
    "\n",
    "def visualize_diversify(\n",
    "        csv_path, \n",
    "        reporters, \n",
    "        train_size, \n",
    "        layer=13, \n",
    "        model=\"meta-llama/Llama-2-13b-hf\", \n",
    "        ignore_train_datasets=[], \n",
    "        ignore_eval_datasets=[], \n",
    "        require_train_datasets=[], \n",
    "        apply_train_examples_per_dataset=False, \n",
    "        seeds=[],\n",
    "        transfer_type=\"any\",\n",
    "        n_datasets_traineds=[]\n",
    "    ):\n",
    "\n",
    "    if type(reporters) is str: reporters = [reporters]\n",
    "    \n",
    "    metric = \"accuracy\"\n",
    "\n",
    "    # Create titles and filename for saving   \n",
    "    pretty_transfer_type = {\n",
    "        \"no transfer\": \"seen\",\n",
    "        \"full transfer\": \"unseen\",\n",
    "        \"semi transfer\": \"related seen\",\n",
    "        \"any\": \"any\"\n",
    "    }\n",
    "    def create_title(reporter, train_size, apply_train_examples_per_dataset, transfer_type):\n",
    "        pretty_reporter = {\n",
    "            \"<class 'probes.LRProbe'>\": \"LR\",\n",
    "            \"<class 'probes.MMProbe'>\": \"MM\",\n",
    "            \"<class 'probes.CCSProbe'>\": \"CCS\",\n",
    "            \"<class 'probes.CrcReporter'>\": \"CRC\",\n",
    "            \"<class 'probes.MMProbe'>_tuple_inference\": \"MM (contrast-inference)\",\n",
    "            \"<class 'probes.LRProbe'>_tuple_inference\": \"LR (contrast-inference)\",\n",
    "        }\n",
    "        aggregation_strategy = \"Fixed Contribution\" if apply_train_examples_per_dataset else \"Fixed Total\"\n",
    "\n",
    "        return f\"Probe: {pretty_reporter[reporter]} on {pretty_transfer_type[transfer_type]} target, n={train_size} ({aggregation_strategy})\"\n",
    "    save_path = Path(csv_path).parent / f\"{train_size}_{pretty_transfer_type[transfer_type]}_{'FC' if apply_train_examples_per_dataset else 'FT'}_{metric}.png\"\n",
    "\n",
    "    eval_only_datasets = [\n",
    "        'got/companies_true_false',\n",
    "        'got/common_claim_true_false',\n",
    "        'got/cities_cities_conj',\n",
    "        'got/cities_cities_disj',\n",
    "    ]\n",
    "    \n",
    "    ordered_datasets = [\n",
    "        'got/cities', \n",
    "        'got/neg_cities', \n",
    "        'got/larger_than',\n",
    "        'got/smaller_than', \n",
    "        'got/sp_en_trans', \n",
    "        'got/neg_sp_en_trans',\n",
    "        'azaria/animals_true_false', \n",
    "        'azaria/neg_animals_true_false',\n",
    "        'azaria/elements_true_false', \n",
    "        'azaria/neg_elements_true_false',\n",
    "        'azaria/facts_true_false', \n",
    "        'azaria/neg_facts_true_false',\n",
    "        'azaria/inventions_true_false', \n",
    "        'azaria/neg_inventions_true_false',\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Constants\n",
    "    colormap = cm.get_cmap('tab20', 20)\n",
    "    oracle_csv_path = Path(r\"experiments\\diversify_remake\\summary_oracle_full.csv\")\n",
    "    always_ignore_datasets = [\"got/counterfact_true\", \"got/counterfact_false\"] # these datasets have been corrupted\n",
    "\n",
    "    # Modify parameters\n",
    "    ignore_eval_datasets += always_ignore_datasets\n",
    "    ignore_train_datasets += always_ignore_datasets\n",
    "\n",
    "    # DataFrame and Filtering\n",
    "    filtered_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Fix wrongly stored data\n",
    "    # Counting medleys used, replace 0->1 only affects oracles\n",
    "    filtered_df[\"n_train_datasets\"] = (filtered_df[\"train_desc\"].str.count(r'\\+')).replace(0,1)\n",
    "\n",
    "    filtered_df = filtered_df[filtered_df[\"model\"] == model]\n",
    "    filtered_df = filtered_df[filtered_df[\"layer\"] == layer]\n",
    "    for ignore_train_dataset in ignore_train_datasets:\n",
    "        filtered_df = filtered_df[~(filtered_df[\"train_desc\"].str.contains(ignore_train_dataset))]\n",
    "    for ignore_eval_dataset in ignore_eval_datasets:\n",
    "        filtered_df = filtered_df[filtered_df[\"eval_dataset\"] != ignore_eval_dataset]\n",
    "    for require_train_dataset in require_train_datasets:\n",
    "        filtered_df = filtered_df[filtered_df[\"train_desc\"].str.contains(require_train_dataset)]\n",
    "\n",
    "    if reporters: \n",
    "        filtered_df = filtered_df[filtered_df[\"reporter\"].isin(reporters)]\n",
    "    if train_size:\n",
    "        if apply_train_examples_per_dataset: \n",
    "            expected_train_sizes = filtered_df[\"n_train_datasets\"] * train_size\n",
    "            filtered_df = filtered_df[filtered_df[\"train_size\"] == expected_train_sizes]\n",
    "        else:\n",
    "            filtered_df = filtered_df[filtered_df[\"train_size\"] == train_size]\n",
    "    if seeds:\n",
    "        filtered_df = filtered_df[filtered_df[\"seed\"].isin(seeds)]\n",
    "    if n_datasets_traineds:\n",
    "        filtered_df = filtered_df[filtered_df[\"n_train_datasets\"].isin(n_datasets_traineds)]\n",
    "    if transfer_type != \"any\":\n",
    "        filtered_df = filtered_df[filtered_df[\"transfer_type\"] == transfer_type]\n",
    "        \n",
    "    # # TEMP\n",
    "    # seed_groups = [(seed, len(group_df)) for (seed,  ), group_df in filtered_df.groupby([\"seed\"])]\n",
    "    # print(seed_groups)\n",
    "\n",
    "    global temp_df\n",
    "    temp_df = filtered_df\n",
    "\n",
    "    # Sort datasets to ensure ordered_datasets come first in the legend\n",
    "    unlisted_datasets = [ds for ds in filtered_df[\"eval_dataset\"].unique() if ds not in ordered_datasets]\n",
    "    sorted_datasets = ordered_datasets + unlisted_datasets\n",
    "    # Remove datasets that do not appear\n",
    "    sorted_datasets = [ds for ds in sorted_datasets if ds in filtered_df[\"eval_dataset\"].unique()]\n",
    "\n",
    "    # Assign colors to datasets, prioritizing the ordered_datasets list\n",
    "    ds_to_color = {eval_dataset: colormap(i) for i, eval_dataset in enumerate(sorted_datasets)}\n",
    "\n",
    "    assert len(filtered_df[(filtered_df[\"oracle\"]) & filtered_df[\"transfer_type\"].str.contains(\"unseen\")]) == 0, \"Oracles should be 'seen'\"\n",
    "\n",
    "    # Load baselines\n",
    "    oracle_df = pd.read_csv(oracle_csv_path)\n",
    "    oracle_df = oracle_df[(oracle_df['oracle']) & (oracle_df['reporter'] == \"<class 'probes.LRProbe'>\")]\n",
    "\n",
    "    groups = filtered_df.groupby([\"reporter\", \"transfer_type\"])\n",
    "    fig, axes = plt.subplots(len(groups), figsize=(7.5,1+5*len(groups)), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "    legend_elements = []\n",
    "    for i, ((reporter, transfer_type), subplot_df) in enumerate(groups):\n",
    "        ax = axes[i]\n",
    "\n",
    "        ax.set_title(create_title(reporter, train_size, apply_train_examples_per_dataset, transfer_type))\n",
    "\n",
    "        n_train_values = sorted(subplot_df[\"n_train_datasets\"].unique())\n",
    "        ax.set_xlabel(\"Training datasets mixed\")\n",
    "        ax.set_xticks(n_train_values)\n",
    "        ax.set_xticklabels(n_train_values)\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_ylim(0.6, 1.02)\n",
    "        ax.set_xlim(0.5, 7.5)\n",
    "\n",
    "        oracle_accuracies = []\n",
    "        all_datasets_df = pd.DataFrame()\n",
    "        # Plot one curve for each eval dataset\n",
    "        for j, (eval_dataset, curve_df) in enumerate(subplot_df.groupby(\"eval_dataset\")):\n",
    "            # To plot one datapoint for each seed and n_train_datasets, we store the accuracy averaged over training configurations\n",
    "            aggs_across_training_configs = []\n",
    "\n",
    "            # Check if all train_descs are uniformly represented\n",
    "            for (n_train_datasets), datapoint_df in curve_df.groupby([\"n_train_datasets\"]):\n",
    "                seeds_per_train_desc = [len(df) for (train_desc), df in datapoint_df.groupby([\"train_desc\"])]\n",
    "                if not all([val == seeds_per_train_desc[0] for val in seeds_per_train_desc]):\n",
    "                    print(f\"WARNING: Some train_descs are under / over represented for {eval_dataset=}{n_train_datasets=}\")\n",
    "\n",
    "            previous_datapoint_train_datasets = None\n",
    "            for (seed, n_train_datasets), datapoint_df in curve_df.groupby([\"seed\", \"n_train_datasets\"]):\n",
    "\n",
    "                # For debug/validation purposes, indicate unfair comparison by drawing black line on this datapoint if different training datasets were used between this run and the previous n_train_datasets or seeds\n",
    "                datapoint_train_datasets = set([dataset for dataset in datapoint_df[\"all_train_datasets\"].unique() for dataset in ast.literal_eval(dataset)])\n",
    "                if previous_datapoint_train_datasets is not None and datapoint_train_datasets != previous_datapoint_train_datasets:\n",
    "                    # There are two cases in which we expect this in the \"no transfer\" setting\n",
    "                    # When going from 1 to 2 train datasets, as that is when new datasets are being added to the mix\n",
    "                    # And when going from multiple to 1 train dataset (usually because of a new seed), as that is when they are removed from the mix again\n",
    "                    if not (transfer_type == \"no transfer\" and (n_train_datasets == 2 or (n_train_datasets == 1 and len(previous_datapoint_train_datasets) > 1))):\n",
    "                        # ax.axvline(x=n_train_datasets, color='black', linewidth=0.5, linestyle=\"--\")\n",
    "                        pass\n",
    "                previous_datapoint_train_datasets = datapoint_train_datasets\n",
    "\n",
    "                aggs_across_training_configs.append({\n",
    "                    \"seed\": seed, \n",
    "                    \"n_train_datasets\": n_train_datasets,\n",
    "                    \"eval_dataset\": eval_dataset,\n",
    "                    \"mean_accuracy\": datapoint_df[metric].mean()\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Compute mean and std for this eval_dataset and each n_train_datasets across seeds\n",
    "            this_dataset_points = []\n",
    "            for n_train_datasets, datapoint_df in pd.DataFrame(aggs_across_training_configs).groupby(\"n_train_datasets\"):\n",
    "                this_dataset_points.append({\n",
    "                    \"n_train_datasets\": n_train_datasets,\n",
    "                    \"eval_dataset\": eval_dataset,\n",
    "                    \"acc_mean\": datapoint_df[\"mean_accuracy\"].mean(), \n",
    "                    \"acc_std\": datapoint_df[\"mean_accuracy\"].std(), \n",
    "                    \"n_seeds\": len(datapoint_df)\n",
    "                })\n",
    "            #     aggs_across_seeds.append((n_train_datasets, datapoint_df[\"mean_accuracy\"].mean(), datapoint_df[\"mean_accuracy\"].std(), len(datapoint_df))) # REMOVE?\n",
    "\n",
    "            eval_ds_color = ds_to_color[eval_dataset]\n",
    "            # n_train_datasets, means, stds, n = zip(*aggs_across_seeds)\n",
    "            # ax.plot(n_train_datasets, means, color=eval_ds_color) # REMOVE?\n",
    "            this_dataset_df = pd.DataFrame(this_dataset_points)\n",
    "            ax.plot(\n",
    "                this_dataset_df[\"n_train_datasets\"], \n",
    "                this_dataset_df[\"acc_mean\"], \n",
    "                color=eval_ds_color)\n",
    "            \n",
    "            all_datasets_df = pd.concat([all_datasets_df, this_dataset_df], ignore_index=True)\n",
    "\n",
    "            # ax.errorbar(n_train_datasets, means, yerr=stds, fmt='-o', color=eval_ds_color, capsize=3, lw=1) # REMOVE?\n",
    "            ax.errorbar(\n",
    "                this_dataset_df[\"n_train_datasets\"], \n",
    "                this_dataset_df[\"acc_mean\"], \n",
    "                yerr=this_dataset_df[\"acc_std\"], \n",
    "                fmt='-o', \n",
    "                color=eval_ds_color, \n",
    "                capsize=3, \n",
    "                lw=1)\n",
    "\n",
    "            # Plot oracle baseline\n",
    "            oracle_accuracy = oracle_df[(oracle_df['eval_dataset'] == eval_dataset) & (oracle_df['n_train_datasets'] == 1)]['accuracy']\n",
    "            # Ignore oracles for which we don't have data\n",
    "            if len(oracle_accuracy) > 0:\n",
    "                ax.axhline(y=oracle_accuracy.item(), color=eval_ds_color, linestyle='-')\n",
    "                oracle_accuracies.append(oracle_accuracy.item())\n",
    "            else:\n",
    "                print(f\"No oracle for {eval_dataset}\")\n",
    "\n",
    "        # # Plot average of seperately trained oracle baseline\n",
    "        # ax.axhline(y=np.mean(oracle_accuracies), color=\"black\", lw=1.0, linestyle=\"--\")\n",
    "\n",
    "        # Plot jointly trained oracle baseline\n",
    "        jointly_trained_oracle_row = oracle_df[oracle_df['n_train_datasets'] == oracle_df['n_train_datasets'].max()]\n",
    "        n_train_for_oracle = jointly_trained_oracle_row['n_train_datasets'].iloc[0]\n",
    "        n_unique_eval_sets = len(subplot_df['eval_dataset'].unique())\n",
    "        if n_train_for_oracle != n_unique_eval_sets:\n",
    "            print(f\"Oracle was trained on {n_train_for_oracle} datasets, but we show {n_unique_eval_sets}.\")\n",
    "        ax.axhline(y=jointly_trained_oracle_row['accuracy'].item(), color=\"black\", lw=2.0, linestyle=\"-\")\n",
    "\n",
    "\n",
    "        # Plot averages in black\n",
    "        # Summarize over medleys\n",
    "        aggs_over_mixes = []\n",
    "        for (seed, n_train_datasets, eval_dataset), sub_df in subplot_df.groupby([\"seed\", \"n_train_datasets\", \"eval_dataset\"]):\n",
    "            # Ignore datasets that are only used for evaluation to allow fair comparison between transfer types\n",
    "            if eval_dataset not in eval_only_datasets:\n",
    "                aggs_over_mixes.append({\n",
    "                    \"n_train_datasets\": n_train_datasets,\n",
    "                    \"eval_dataset\": eval_dataset,\n",
    "                    \"acc_mean\": sub_df[metric].mean(), \n",
    "                    # \"acc_std_over_mixes\": sub_df[\"accuracy\"].std(), \n",
    "                    \"seed\": seed\n",
    "                })\n",
    "\n",
    "        summarized_df = pd.DataFrame(aggs_over_mixes)\n",
    "\n",
    "        # For average dataframe, first aggregate over medleys and eval datasets\n",
    "        agg_rows = []\n",
    "        for (n_train_datasets, seed), sub_df in summarized_df.groupby([\"n_train_datasets\", \"seed\"]):\n",
    "            agg_rows.append({\n",
    "                \"n_train_datasets\": n_train_datasets,\n",
    "                \"acc_mean\": sub_df[\"acc_mean\"].mean(), \n",
    "                # \"acc_std\": sub_df[\"acc_std\"].std(),\n",
    "                \"seed\": seed\n",
    "            })\n",
    "        agg_df = pd.DataFrame(agg_rows)\n",
    "\n",
    "        # Then further aggregate over seeds to obtain stds across seeds (but not eval_datasets etc.)\n",
    "        agg_rows = []\n",
    "        for (n_train_datasets,), sub_df in agg_df.groupby([\"n_train_datasets\"]):\n",
    "            agg_rows.append({\n",
    "                \"n_train_datasets\": n_train_datasets,\n",
    "                \"acc_mean\": sub_df[\"acc_mean\"].mean(), \n",
    "                \"acc_std\": sub_df[\"acc_mean\"].std(),\n",
    "            })\n",
    "        agg_df = pd.DataFrame(agg_rows)\n",
    "\n",
    "        # Plot averages\n",
    "        ax.plot(agg_df[\"n_train_datasets\"], agg_df[\"acc_mean\"], color=\"black\", lw=3)\n",
    "        ax.errorbar(agg_df[\"n_train_datasets\"], agg_df[\"acc_mean\"], yerr=agg_df[\"acc_std\"], fmt='-o', color=\"black\", capsize=3, lw=1)\n",
    "\n",
    "\n",
    "    # Create custom legend\n",
    "    def format_dataset_name(ds_name, eval_only_datasets):\n",
    "        if ds_name in eval_only_datasets:\n",
    "            return f\"({ds_name})\"\n",
    "        return ds_name\n",
    "    \n",
    "    legend_elements.append(Line2D([0], [0], color=\"black\", lw=3, label=\"average\"))\n",
    "    legend_elements.extend([Line2D([0], [0], color=ds_to_color[ds], lw=2, label=format_dataset_name(ds, eval_only_datasets)) for ds in sorted_datasets])\n",
    "    axes[0].legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig(save_path)\n",
    "    print(f\"Saved fig to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data from different seeds into one file per setup\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Directory where the CSV files are located\n",
    "input_dir = r\"experiments\\diversify_remake\\thesis_summaries\"\n",
    "output_dir = r\"experiments\\diversify_remake\\thesis_summaries_merged\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get a list of all CSV files in the input directory\n",
    "csv_files = glob(os.path.join(input_dir, '*.csv'))\n",
    "\n",
    "# Group files by their common prefix (excluding the seed)\n",
    "file_groups = {}\n",
    "for file in csv_files:\n",
    "    # Extract the common prefix by removing the seed part\n",
    "    prefix = '_'.join(file.split('_')[:-1])\n",
    "    if prefix not in file_groups:\n",
    "        file_groups[prefix] = []\n",
    "    file_groups[prefix].append(file)\n",
    "\n",
    "# Merge files for each group and save to a new CSV file\n",
    "for prefix, files in file_groups.items():\n",
    "    # Read and concatenate all files in the group\n",
    "    merged_df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "    # Define the output file path\n",
    "    output_file = os.path.join(output_dir, os.path.basename(prefix) + '_merged.csv')\n",
    "    # Save the merged DataFrame to a CSV file\n",
    "    # merged_df.to_csv(output_file, index=False)\n",
    "    unique_probes_df = merged_df.drop_duplicates((\"reporter\", \"train_desc\", \"train_size\", \"oracle\", \"seed\"))\n",
    "    print(f\"{len(merged_df)} evaluations from {len(unique_probes_df)} probes in {output_file}\")\n",
    "\n",
    "print(f\"Files have been merged and saved to '{output_dir}' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def prepare_diversify(\n",
    "        csv_path, \n",
    "        csv_out,\n",
    "        reporters, \n",
    "        train_size, \n",
    "        layer=13, \n",
    "        model=\"meta-llama/Llama-2-13b-hf\", \n",
    "        ignore_train_datasets=[], \n",
    "        ignore_eval_datasets=[], \n",
    "        require_train_datasets=[], \n",
    "        apply_train_examples_per_dataset=False, \n",
    "        seeds=[],\n",
    "        transfer_type=\"any\",\n",
    "        n_datasets_traineds=[]\n",
    "    ):\n",
    "\n",
    "    if type(reporters) is str:\n",
    "        reporters = [reporters]\n",
    "    \n",
    "    debug = True\n",
    "    metric = \"accuracy\"\n",
    "\n",
    "    # Constants\n",
    "    oracle_csv_path = Path(r\"experiments\\diversify_remake\\summary_oracle_full.csv\")\n",
    "    always_ignore_datasets = [\"got/counterfact_true\", \"got/counterfact_false\"]\n",
    "\n",
    "    # Modify parameters\n",
    "    ignore_eval_datasets += always_ignore_datasets\n",
    "    ignore_train_datasets += always_ignore_datasets\n",
    "\n",
    "    # DataFrame and Filtering\n",
    "    filtered_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Fix wrongly stored data\n",
    "    filtered_df[\"n_train_datasets\"] = (filtered_df[\"train_desc\"].str.count(r'\\+')).replace(0,1)\n",
    "\n",
    "    filtered_df = filtered_df[filtered_df[\"model\"] == model]\n",
    "    filtered_df = filtered_df[filtered_df[\"layer\"] == layer]\n",
    "    for ignore_train_dataset in ignore_train_datasets:\n",
    "        filtered_df = filtered_df[~(filtered_df[\"train_desc\"].str.contains(ignore_train_dataset))]\n",
    "    for ignore_eval_dataset in ignore_eval_datasets:\n",
    "        filtered_df = filtered_df[filtered_df[\"eval_dataset\"] != ignore_eval_dataset]\n",
    "    for require_train_dataset in require_train_datasets:\n",
    "        filtered_df = filtered_df[filtered_df[\"train_desc\"].str.contains(require_train_dataset)]\n",
    "\n",
    "    if debug: print(\"Unique n_train_datasets after initial filtering:\", filtered_df[\"n_train_datasets\"].unique())\n",
    "\n",
    "    if reporters: \n",
    "        filtered_df = filtered_df[filtered_df[\"reporter\"].isin(reporters)]\n",
    "    if debug: print(\"Unique n_train_datasets after reporter filter:\", filtered_df[\"n_train_datasets\"].unique())\n",
    "    if train_size:\n",
    "        if apply_train_examples_per_dataset: \n",
    "            expected_train_sizes = filtered_df[\"n_train_datasets\"] * train_size\n",
    "            filtered_df = filtered_df[filtered_df[\"train_size\"] == expected_train_sizes]\n",
    "        else:\n",
    "            filtered_df = filtered_df[filtered_df[\"train_size\"] == train_size]\n",
    "    if debug: print(\"Unique n_train_datasets after train_size filter:\", filtered_df[\"n_train_datasets\"].unique())\n",
    "    if seeds:\n",
    "        filtered_df = filtered_df[filtered_df[\"seed\"].isin(seeds)]\n",
    "    if debug: print(\"Unique n_train_datasets after seeds filter:\", filtered_df[\"n_train_datasets\"].unique())\n",
    "    if n_datasets_traineds:\n",
    "        filtered_df = filtered_df[filtered_df[\"n_train_datasets\"].isin(n_datasets_traineds)]\n",
    "    if debug: print(\"Unique n_train_datasets after n_datasets_traineds filter:\", filtered_df[\"n_train_datasets\"].unique())\n",
    "    if transfer_type != \"any\":\n",
    "        filtered_df = filtered_df[filtered_df[\"transfer_type\"] == transfer_type]\n",
    "    if debug: print(\"Unique n_train_datasets after transfer_type filter:\", filtered_df[\"n_train_datasets\"].unique())\n",
    "\n",
    "    # Load baselines\n",
    "    oracle_df = pd.read_csv(oracle_csv_path)\n",
    "    oracle_df = oracle_df[(oracle_df['oracle']) & (oracle_df['reporter'] == \"<class 'probes.LRProbe'>\")]\n",
    "\n",
    "    # Prepare data for visualization\n",
    "    if debug: print(\"Final unique n_train_datasets before processing:\", filtered_df[\"n_train_datasets\"].unique())\n",
    "\n",
    "    all_data = []\n",
    "    avg_data = []\n",
    "\n",
    "    eval_only_datasets = [\n",
    "        'got/companies_true_false',\n",
    "        'got/common_claim_true_false',\n",
    "        'got/cities_cities_conj',\n",
    "        'got/cities_cities_disj',\n",
    "    ]\n",
    "\n",
    "    for (reporter, transfer_type), group_df in filtered_df.groupby([\"reporter\", \"transfer_type\"]):\n",
    "        for (n_train_datasets, eval_dataset), subgroup_df in group_df.groupby([\"n_train_datasets\", \"eval_dataset\"]):\n",
    "            # First level of aggregation: over training configurations\n",
    "            agg_over_configs = subgroup_df.groupby(\"seed\")[metric].mean().reset_index()\n",
    "            \n",
    "            # Second level of aggregation: over seeds\n",
    "            agg_over_seeds = agg_over_configs[metric].agg([\"mean\", \"std\"])\n",
    "            \n",
    "            all_data.append({\n",
    "                \"reporter\": reporter,\n",
    "                \"transfer_type\": transfer_type,\n",
    "                \"aggregation_strategy\": \"Fixed Contribution\" if apply_train_examples_per_dataset else \"Fixed Total\",\n",
    "                \"n_train_datasets\": n_train_datasets,\n",
    "                \"eval_dataset\": eval_dataset,\n",
    "                \"acc_mean\": agg_over_seeds[\"mean\"],\n",
    "                \"acc_std\": agg_over_seeds[\"std\"] if len(agg_over_configs) > 1 else 0,\n",
    "                \"n_seeds\": len(agg_over_configs)\n",
    "            })\n",
    "\n",
    "        # Calculate average over non-eval-only datasets\n",
    "        for n_train_datasets, subgroup_df in group_df[~group_df[\"eval_dataset\"].isin(eval_only_datasets)].groupby(\"n_train_datasets\"):\n",
    "            # First level of aggregation: over training configurations and eval datasets\n",
    "            agg_over_configs_and_datasets = subgroup_df.groupby(\"seed\")[metric].mean().reset_index()\n",
    "            \n",
    "            # Second level of aggregation: over seeds\n",
    "            agg_over_seeds = agg_over_configs_and_datasets[metric].agg([\"mean\", \"std\"])\n",
    "            \n",
    "            avg_data.append({\n",
    "                \"reporter\": reporter,\n",
    "                \"transfer_type\": transfer_type,\n",
    "                \"aggregation_strategy\": \"Fixed Contribution\" if apply_train_examples_per_dataset else \"Fixed Total\",\n",
    "                \"n_train_datasets\": n_train_datasets,\n",
    "                \"eval_dataset\": \"average\",\n",
    "                \"acc_mean\": agg_over_seeds[\"mean\"],\n",
    "                \"acc_std\": agg_over_seeds[\"std\"] if len(agg_over_configs_and_datasets) > 1 else 0,\n",
    "                \"n_seeds\": len(agg_over_configs_and_datasets)\n",
    "            })\n",
    "\n",
    "    # Combine individual dataset results and overall average\n",
    "    final_data = pd.DataFrame(all_data + avg_data)\n",
    "\n",
    "    # Save processed data\n",
    "    final_data.to_csv(csv_out, index=False)\n",
    "    oracle_csv_out = Path(str(csv_out).replace('.csv', '_oracle.csv'))\n",
    "    oracle_df.to_csv(oracle_csv_out, index=False)\n",
    "\n",
    "    if debug:\n",
    "        print(\"Unique n_train_datasets in processed data:\", final_data[\"n_train_datasets\"].unique())\n",
    "\n",
    "    print(f\"Saved processed data to {csv_out} and {oracle_csv_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data from experiments, compute averages\n",
    "import os\n",
    "\n",
    "# Preparing data (not visualizing)\n",
    "# Settings for all figures\n",
    "csv_dir = Path(r\"experiments\\diversify_remake\\thesis_summaries_merged\")\n",
    "csv_plot_data_dir = csv_dir / \"plot_data\"\n",
    "os.makedirs(csv_plot_data_dir, exist_ok=True)\n",
    "\n",
    "# Common settings\n",
    "ignore_train_datasets = [\"got/companies_true_false\"]\n",
    "\n",
    "filename = \"summary_500_total_merged.csv\"\n",
    "apply_train_examples_per_dataset=False\n",
    "filename_out = filename\n",
    "prepare_diversify(\n",
    "    csv_dir / filename,\n",
    "    csv_plot_data_dir / filename_out,\n",
    "    reporters=None,\n",
    "    train_size=500,\n",
    "    ignore_train_datasets=ignore_train_datasets,\n",
    "    apply_train_examples_per_dataset=apply_train_examples_per_dataset,\n",
    "    transfer_type=\"any\",\n",
    "    n_datasets_traineds=[1,2,3,4,5,6,7]\n",
    ")\n",
    "\n",
    "filename = \"summary_250_contrib_merged.csv\"\n",
    "apply_train_examples_per_dataset=True\n",
    "filename_out = filename.replace(\"250\", \"500\") # fixing misnamed file. train_size argument below guarantees correctness\n",
    "prepare_diversify(\n",
    "    csv_dir / filename,\n",
    "    csv_plot_data_dir / filename_out,\n",
    "    reporters=None,\n",
    "    train_size=500,\n",
    "    ignore_train_datasets=ignore_train_datasets,\n",
    "    apply_train_examples_per_dataset=apply_train_examples_per_dataset,\n",
    "    transfer_type=\"any\",\n",
    "    n_datasets_traineds=[1,2,3,4,5,6,7]\n",
    ")\n",
    "\n",
    "filename = \"summary_500_contrib_merged.csv\"\n",
    "apply_train_examples_per_dataset=True\n",
    "filename_out = filename.replace(\"500\", \"1000\") # fixing misnamed file. train_size argument below guarantees correctness\n",
    "prepare_diversify(\n",
    "    csv_dir / filename,\n",
    "    csv_plot_data_dir / filename_out,\n",
    "    reporters=None,\n",
    "    train_size=1000,\n",
    "    ignore_train_datasets=ignore_train_datasets,\n",
    "    apply_train_examples_per_dataset=apply_train_examples_per_dataset,\n",
    "    transfer_type=\"any\",\n",
    "    n_datasets_traineds=[1,2,3,4,5,6,7]\n",
    ")\n",
    "\n",
    "filename = \"summary_1000_total_merged.csv\"\n",
    "apply_train_examples_per_dataset=False\n",
    "filename_out = filename\n",
    "prepare_diversify(\n",
    "    csv_dir / filename,\n",
    "    csv_plot_data_dir / filename_out,\n",
    "    reporters=None,\n",
    "    train_size=1000,\n",
    "    ignore_train_datasets=ignore_train_datasets,\n",
    "    apply_train_examples_per_dataset=apply_train_examples_per_dataset,\n",
    "    # transfer_type=\"full transfer\",\n",
    "    n_datasets_traineds=[2,3,4,5,6]\n",
    ")\n",
    "\n",
    "# UNSUPERVISED\n",
    "# For unsupervised methods we store the number of pairs, which is half as many. \n",
    "# We still name it the same downstream from here, as it is effectively the same number of samples\n",
    "unsupervised_train_size_multiplier=0.5\n",
    "\n",
    "# Figure 1 a: 500, Unseen, FT\n",
    "filename = \"ccs_summary_500_total_merged.csv\"\n",
    "apply_train_examples_per_dataset=False\n",
    "filename_out = filename\n",
    "prepare_diversify(\n",
    "    csv_dir / filename,\n",
    "    csv_plot_data_dir / filename_out,\n",
    "    reporters=None,\n",
    "    train_size=500*unsupervised_train_size_multiplier,\n",
    "    ignore_train_datasets=ignore_train_datasets,\n",
    "    apply_train_examples_per_dataset=apply_train_examples_per_dataset,\n",
    "    transfer_type=\"any\",\n",
    "    n_datasets_traineds=[1,2,3,4,5,6,7]\n",
    ")\n",
    "\n",
    "filename = \"ccs_summary_500_contrib_merged.csv\"\n",
    "apply_train_examples_per_dataset=True\n",
    "filename_out = filename # fixing misnamed file. train_size argument below guarantees correctness\n",
    "prepare_diversify(\n",
    "    csv_dir / filename,\n",
    "    csv_plot_data_dir / filename_out,\n",
    "    reporters=None,\n",
    "    train_size=500*unsupervised_train_size_multiplier,\n",
    "    ignore_train_datasets=ignore_train_datasets,\n",
    "    apply_train_examples_per_dataset=apply_train_examples_per_dataset,\n",
    "    transfer_type=\"any\",\n",
    "    n_datasets_traineds=[1,2,3,4,5,6,7]\n",
    ")\n",
    "\n",
    "filename = \"ccs_summary_1000_total_merged.csv\"\n",
    "apply_train_examples_per_dataset=False\n",
    "filename_out = filename\n",
    "prepare_diversify(\n",
    "    csv_dir / filename,\n",
    "    csv_plot_data_dir / filename_out,\n",
    "    reporters=None,\n",
    "    train_size=1000*unsupervised_train_size_multiplier,\n",
    "    ignore_train_datasets=ignore_train_datasets,\n",
    "    apply_train_examples_per_dataset=apply_train_examples_per_dataset,\n",
    "    # transfer_type=\"full transfer\",\n",
    "    n_datasets_traineds=[2,3,4,5,6]\n",
    ")\n",
    "\n",
    "# Require sp_en_trans\n",
    "filename = \"summary_500_total_merged.csv\"\n",
    "apply_train_examples_per_dataset=False\n",
    "required_train_datasets = [\"got/sp_en_trans\"]\n",
    "filename_out = filename.replace(\".csv\", \"_spanish.csv\")\n",
    "prepare_diversify(\n",
    "    csv_dir / filename,\n",
    "    csv_plot_data_dir / filename_out,\n",
    "    reporters=None,\n",
    "    train_size=500,\n",
    "    ignore_train_datasets=ignore_train_datasets,\n",
    "    require_train_datasets=required_train_datasets,\n",
    "    apply_train_examples_per_dataset=apply_train_examples_per_dataset,\n",
    "    # transfer_type=\"full transfer\",\n",
    "    n_datasets_traineds=[1,2,3,4,5,6]\n",
    ")\n",
    "\n",
    "filename = \"summary_250_contrib_merged.csv\"\n",
    "required_train_datasets = [\"got/sp_en_trans\"]\n",
    "apply_train_examples_per_dataset=True\n",
    "filename_out = filename.replace(\"250\", \"500\").replace(\".csv\", \"_spanish.csv\")\n",
    "prepare_diversify(\n",
    "    csv_dir / filename,\n",
    "    csv_plot_data_dir / filename_out,\n",
    "    reporters=None,\n",
    "    train_size=500,\n",
    "    ignore_train_datasets=ignore_train_datasets,\n",
    "    require_train_datasets=required_train_datasets,\n",
    "    apply_train_examples_per_dataset=apply_train_examples_per_dataset,\n",
    "    # transfer_type=\"full transfer\",\n",
    "    n_datasets_traineds=[1,2,3,4,5,6]\n",
    ")\n",
    "\n",
    "# Contrastive inference\n",
    "filename = \"summary_500_tuple_inference_merged.csv\"\n",
    "reporters = [\n",
    "    \"<class 'probes.LRProbe'>_tuple_inference\",\n",
    "    \"<class 'probes.MMProbe'>_tuple_inference\"\n",
    "    ]\n",
    "ignore_eval_datasets=[\n",
    "            'got/companies_true_false',\n",
    "            'got/common_claim_true_false',\n",
    "            'got/cities_cities_conj',\n",
    "            'got/cities_cities_disj'\n",
    "        ]\n",
    "apply_train_examples_per_dataset = False\n",
    "filename_out = filename\n",
    "prepare_diversify(\n",
    "    csv_dir / filename, \n",
    "    csv_plot_data_dir / filename_out,\n",
    "    reporters, \n",
    "    train_size=None, # For this particular figure, the train sizes range from 496 - 504. We accept this inaccuracy and do not filter to not lose any data \n",
    "    layer=13, \n",
    "    model=\"meta-llama/Llama-2-13b-hf\", \n",
    "    ignore_train_datasets=['got/companies_true_false'], \n",
    "    ignore_eval_datasets=ignore_eval_datasets, \n",
    "    require_train_datasets=[], \n",
    "    apply_train_examples_per_dataset=apply_train_examples_per_dataset, \n",
    "    seeds=[],\n",
    "    transfer_type=\"any\",\n",
    "    n_datasets_traineds=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data from different experimental setups so they can be in one CSV and one Figure\n",
    "def prepare_combined_data(csv_dir, output_csv_path, configs):   \n",
    "    all_data = []\n",
    "   \n",
    "    for filename, train_size in configs:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_dir / filename)\n",
    "            df[\"train_size\"] = train_size\n",
    "            all_data.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File {filename} not found. Skipping this configuration.\")\n",
    "      \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "    os.makedirs(output_csv_path.parent, exist_ok=True)\n",
    "    combined_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Combined data saved to {output_csv_path}\")\n",
    "\n",
    "working_dir = Path(r\"experiments\\diversify_remake\\thesis_summaries_merged\")\n",
    "csv_dir = working_dir / \"plot_data\"\n",
    "combined_dir = working_dir / \"combined\"\n",
    "\n",
    "# Default experiments n=500\n",
    "combined_csv_path = combined_dir / \"combined_data_500.csv\"\n",
    "configs = [\n",
    "    (\"summary_500_total_merged.csv\", 500),\n",
    "    (\"summary_500_contrib_merged.csv\", 500),\n",
    "    (\"ccs_summary_500_total_merged.csv\", 500),\n",
    "    (\"ccs_summary_500_contrib_merged.csv\", 500),\n",
    "]\n",
    "prepare_combined_data(csv_dir, combined_csv_path, configs)\n",
    "\n",
    "# Default experiments n=1000\n",
    "combined_csv_path = combined_dir / \"combined_data_1000.csv\"\n",
    "configs = [\n",
    "    (\"summary_1000_total_merged.csv\", 1000),\n",
    "    (\"summary_1000_contrib_merged.csv\", 1000),\n",
    "    (\"ccs_summary_1000_total_merged.csv\", 1000),\n",
    "]\n",
    "prepare_combined_data(csv_dir, combined_csv_path, configs)\n",
    "\n",
    "# Require Spanish\n",
    "combined_csv_path = combined_dir / \"combined_data_spanish.csv\"\n",
    "configs = [\n",
    "    (\"summary_500_contrib_merged_spanish.csv\", 500),\n",
    "    (\"summary_500_total_merged_spanish.csv\", 500),\n",
    "]\n",
    "prepare_combined_data(csv_dir, combined_csv_path, configs)\n",
    "\n",
    "# Contrastive Inference\n",
    "combined_csv_path = combined_dir / \"combined_data_contrastive.csv\"\n",
    "configs = [\n",
    "    (\"summary_500_tuple_inference_merged.csv\", 500),\n",
    "]\n",
    "# Prepare combined data (only need to do this once)\n",
    "prepare_combined_data(csv_dir, combined_csv_path, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_combined(\n",
    "        csv_path, \n",
    "        csv_oracle_path, \n",
    "        reporter, \n",
    "        train_size, \n",
    "        transfer_types=[\"full transfer\", \"no transfer\"], \n",
    "        aggregation_strategies=[\"Fixed Total\", \"Fixed Contribution\"], \n",
    "        contrastive_inference=False,\n",
    "        reference_csv_path=None,\n",
    "        reference_reporter=None):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[df[\"reporter\"] == reporter]\n",
    "    df = df[df[\"train_size\"] == train_size]\n",
    "    oracle_df = pd.read_csv(csv_oracle_path)\n",
    "    oracle_df = oracle_df[(oracle_df[\"oracle\"]) & (oracle_df[\"reporter\"] == \"<class 'probes.LRProbe'>\")]\n",
    "    \n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No data found for reporter {reporter}\")\n",
    "    \n",
    "    # Load reference data if provided\n",
    "    if reference_csv_path:\n",
    "        assert reference_reporter is not None\n",
    "        ref_df = pd.read_csv(reference_csv_path)\n",
    "        ref_df = ref_df[ref_df[\"reporter\"] == reference_reporter]\n",
    "        ref_df = ref_df[ref_df[\"train_size\"] == train_size]\n",
    "        \n",
    "    eval_only_datasets = [\n",
    "        'got/companies_true_false',\n",
    "        'got/common_claim_true_false',\n",
    "        'got/cities_cities_conj',\n",
    "        'got/cities_cities_disj',\n",
    "    ]\n",
    "   \n",
    "    if contrastive_inference:       \n",
    "        ordered_datasets = [\n",
    "            'got/cities+got/neg_cities',\n",
    "            'got/larger_than+got/smaller_than',\n",
    "            'got/sp_en_trans+got/neg_sp_en_trans',\n",
    "            'azaria/animals_true_false+azaria/neg_animals_true_false',\n",
    "            'azaria/elements_true_false+azaria/neg_elements_true_false',\n",
    "            'azaria/facts_true_false+azaria/neg_facts_true_false',\n",
    "            'azaria/inventions_true_false+azaria/neg_inventions_true_false',\n",
    "        ]\n",
    "    else:\n",
    "        ordered_datasets = [\n",
    "            'got/cities',\n",
    "            'got/neg_cities',\n",
    "            'got/larger_than',\n",
    "            'got/smaller_than',\n",
    "            'got/sp_en_trans',\n",
    "            'got/neg_sp_en_trans',\n",
    "            'azaria/animals_true_false',\n",
    "            'azaria/neg_animals_true_false',\n",
    "            'azaria/elements_true_false',\n",
    "            'azaria/neg_elements_true_false',\n",
    "            'azaria/facts_true_false',\n",
    "            'azaria/neg_facts_true_false',\n",
    "            'azaria/inventions_true_false',\n",
    "            'azaria/neg_inventions_true_false',\n",
    "            'got/companies_true_false',\n",
    "            'got/common_claim_true_false',\n",
    "            'got/cities_cities_conj',\n",
    "            'got/cities_cities_disj'\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "    # Create titles and filename for saving   \n",
    "    pretty_transfer_type = {\n",
    "        \"no transfer\": \"seen\",\n",
    "        \"full transfer\": \"unseen\",\n",
    "        \"semi transfer\": \"related seen\",\n",
    "        \"any\": \"any\"\n",
    "    }\n",
    "\n",
    "    def create_title(reporter, train_size):\n",
    "        pretty_reporter = {\n",
    "            \"<class 'probes.LRProbe'>\": \"LR\",\n",
    "            \"<class 'probes.MMProbe'>\": \"MM\",\n",
    "            \"<class 'probes.CCSProbe'>\": \"CCS\",\n",
    "            \"<class 'probes.CrcReporter'>\": \"CRC\",\n",
    "            \"<class 'probes.MMProbe'>_tuple_inference\": \"MM (contrast-inference)\",\n",
    "            \"<class 'probes.LRProbe'>_tuple_inference\": \"LR (contrast-inference)\",\n",
    "        }\n",
    "\n",
    "        return f\"{pretty_reporter[reporter]}, n={train_size}\"\n",
    "        return f\"{pretty_reporter[reporter]} on {pretty_transfer_type[transfer_type]} target, n={train_size} ({aggregation_strategy})\"\n",
    "    \n",
    "    fig, axes = plt.subplots(len(transfer_types), len(aggregation_strategies), figsize=(3+ 6*len(aggregation_strategies), 3 + 6*len(transfer_types)))\n",
    "    axes = axes.reshape((len(aggregation_strategies), (len(transfer_types)))) \n",
    "    fig.suptitle(create_title(reporter, train_size), fontsize=20, y=0.998)\n",
    "    \n",
    "    # Create a custom color palette using tab20\n",
    "    colormap = cm.get_cmap('tab20', 20)\n",
    "    color_dict = {eval_dataset: colormap(i) for i, eval_dataset in enumerate(ordered_datasets)}\n",
    "    color_dict['average'] = 'black'\n",
    "\n",
    "    # Define x-axis values for each subplot\n",
    "    x_axis_values = {\n",
    "        (500, \"full transfer\", \"Fixed Total\"): [1,2,3,4,5,6],\n",
    "        (500, \"full transfer\", \"Fixed Contribution\"): [1,2,3,4,5,6],\n",
    "        (500, \"no transfer\", \"Fixed Total\"): [1,2,3,4,5,6,7],\n",
    "        (500, \"no transfer\", \"Fixed Contribution\"): [1,2,3,4,5,6,7],\n",
    "        (1000, \"full transfer\", \"Fixed Total\"): [2,3,4,5,6],\n",
    "        # (1000, \"full transfer\", \"Fixed Contribution\"): [2,3,4,5,6], # Not enough samples\n",
    "        (1000, \"no transfer\", \"Fixed Total\"): [2,3,4,5,6,7],\n",
    "        # (1000, \"no transfer\", \"Fixed Contribution\"): [2,3,4,5,6,7] # Not enough samples\n",
    "    }\n",
    "\n",
    "\n",
    "    legend_handles = []\n",
    "\n",
    "    # Initialize variables to track overall min and max\n",
    "    overall_y_min = float('inf')\n",
    "    overall_y_max = float('-inf')\n",
    "\n",
    "    for i, transfer_type in enumerate(transfer_types):\n",
    "        for j, agg_strategy in enumerate(aggregation_strategies):\n",
    "            ax = axes[j, i]\n",
    "            data = df[(df[\"transfer_type\"] == transfer_type) & (df[\"aggregation_strategy\"] == agg_strategy)]\n",
    "            \n",
    "            if data.empty:\n",
    "                ax.text(0.5, 0.5, \"No data available\", ha='center', va='center')\n",
    "                continue\n",
    "            \n",
    "            subplot_x_values = x_axis_values[(train_size, transfer_type, agg_strategy)]\n",
    "            \n",
    "            subplot_y_min = float('inf')\n",
    "            subplot_y_max = float('-inf')\n",
    "\n",
    "            # Plot individual datasets\n",
    "            for eval_dataset in ordered_datasets:\n",
    "\n",
    "                subset = data[data[\"eval_dataset\"] == eval_dataset]\n",
    "                subset = subset[subset[\"n_train_datasets\"].isin(subplot_x_values)]\n",
    "                line = ax.plot(subset[\"n_train_datasets\"], subset[\"acc_mean\"], \n",
    "                            color=color_dict[eval_dataset], alpha=0.9, linewidth=2)\n",
    "                ax.fill_between(subset[\"n_train_datasets\"], \n",
    "                                subset[\"acc_mean\"] - subset[\"acc_std\"],\n",
    "                                subset[\"acc_mean\"] + subset[\"acc_std\"],\n",
    "                                color=color_dict[eval_dataset], alpha=0.2)\n",
    "                \n",
    "                # Add error bars (whiskers)\n",
    "                ax.errorbar(subset[\"n_train_datasets\"], subset[\"acc_mean\"], \n",
    "                            yerr=subset[\"acc_std\"], \n",
    "                            fmt='none', ecolor=color_dict[eval_dataset], \n",
    "                            elinewidth=1, capsize=3, alpha=0.7)\n",
    "                \n",
    "                # Update subplot min and max\n",
    "                subplot_y_min = min(subplot_y_min, (subset[\"acc_mean\"] - subset[\"acc_std\"]).min())\n",
    "                subplot_y_max = max(subplot_y_max, (subset[\"acc_mean\"] + subset[\"acc_std\"]).max())\n",
    "\n",
    "                # Plot oracle baseline for individual datasets\n",
    "                oracle_accuracy = oracle_df[\n",
    "                    (oracle_df['eval_dataset'] == eval_dataset) \n",
    "                    & (oracle_df['n_train_datasets'] == 1)\n",
    "                ]['accuracy']\n",
    "                if len(oracle_accuracy) > 0:\n",
    "                    ax.axhline(y=oracle_accuracy.iloc[0], color=color_dict[eval_dataset], linestyle='-', alpha=0.5)\n",
    "\n",
    "                # Only add to legend_handles if it's the first subplot\n",
    "                if i == 0 and j == 0:\n",
    "                    if eval_dataset in eval_only_datasets:\n",
    "                        legend_handles.append(Line2D([0], [0], color=color_dict[eval_dataset], \n",
    "                                                    lw=2, label=f\"({eval_dataset})\"))\n",
    "                    else:\n",
    "                        legend_handles.append(Line2D([0], [0], color=color_dict[eval_dataset], \n",
    "                                                    lw=2, label=eval_dataset))\n",
    "\n",
    "            # Plot average\n",
    "            avg_data = data[data[\"eval_dataset\"] == \"average\"]\n",
    "            avg_data = avg_data[avg_data[\"n_train_datasets\"].isin(subplot_x_values)]\n",
    "            if not avg_data.empty:\n",
    "                ax.plot(avg_data[\"n_train_datasets\"], avg_data[\"acc_mean\"], \n",
    "                        color='black', linewidth=3, label='Average')\n",
    "                ax.fill_between(avg_data[\"n_train_datasets\"], \n",
    "                                avg_data[\"acc_mean\"] - avg_data[\"acc_std\"],\n",
    "                                avg_data[\"acc_mean\"] + avg_data[\"acc_std\"],\n",
    "                                color='black', alpha=0.2)\n",
    "                \n",
    "                # Add error bars (whiskers) for average\n",
    "                ax.errorbar(avg_data[\"n_train_datasets\"], avg_data[\"acc_mean\"], \n",
    "                            yerr=avg_data[\"acc_std\"], \n",
    "                            fmt='none', ecolor='black', \n",
    "                            elinewidth=1, capsize=3, alpha=0.7)\n",
    "            \n",
    "                # Add average to legend only once\n",
    "                if i == 0 and j == 0:\n",
    "                    legend_handles.insert(0, Line2D([0], [0], color='black', lw=3, label='Average'))\n",
    "\n",
    "            # Plot reference average if provided\n",
    "            if reference_csv_path:\n",
    "                ref_avg_data = ref_df[(ref_df[\"transfer_type\"] == transfer_type) & \n",
    "                                      (ref_df[\"aggregation_strategy\"] == agg_strategy) & \n",
    "                                      (ref_df[\"eval_dataset\"] == \"average\")]\n",
    "                ref_avg_data = ref_avg_data[ref_avg_data[\"n_train_datasets\"].isin(subplot_x_values)]\n",
    "                if not ref_avg_data.empty:\n",
    "                    ax.plot(ref_avg_data[\"n_train_datasets\"], ref_avg_data[\"acc_mean\"], \n",
    "                            color='black', linestyle=':', linewidth=2, label='Average (normal inference)')\n",
    "                    \n",
    "                    # Add reference average to legend only once\n",
    "                    if i == 0 and j == 0:\n",
    "                        legend_handles.insert(1, Line2D([0], [0], color='black', linestyle=':', lw=2, label='Average (normal inference)'))\n",
    "    \n",
    "            # Update subplot min and max\n",
    "            subplot_y_min = min(subplot_y_min, (avg_data[\"acc_mean\"] - avg_data[\"acc_std\"]).min())\n",
    "            subplot_y_max = max(subplot_y_max, (avg_data[\"acc_mean\"] + avg_data[\"acc_std\"]).max())\n",
    "\n",
    "            \n",
    "            # Plot oracle baseline for average (black horizontal line)\n",
    "            max_n_train_datasets = oracle_df['n_train_datasets'].max()\n",
    "            oracle_accuracy_avg = oracle_df[oracle_df['n_train_datasets'] == max_n_train_datasets]['accuracy']\n",
    "            if len(oracle_accuracy_avg) > 0:\n",
    "                ax.axhline(y=oracle_accuracy_avg.iloc[0], color='black', linestyle='-', linewidth=2)\n",
    "\n",
    "            ax.set_title(f\"{'Unseen' if transfer_type == 'full transfer' else 'Seen'}, {agg_strategy}\", fontsize=16)\n",
    "            ax.set_xlabel(\"Number of Training Datasets\", fontsize=14)\n",
    "            ax.set_ylabel(\"Accuracy\", fontsize=14)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "            \n",
    "            # Set x-axis limits and ticks\n",
    "            ax.set_xlim(1, 7)\n",
    "            ax.set_xticks(range(1, 8))\n",
    "            ax.set_xticklabels(range(1, 8))\n",
    "\n",
    "            # Update overall min and max\n",
    "            overall_y_min = min(overall_y_min, subplot_y_min)\n",
    "            overall_y_max = max(overall_y_max, subplot_y_max)\n",
    "\n",
    "    # Add some padding to the y-limits\n",
    "    y_range = overall_y_max - overall_y_min\n",
    "    overall_y_min = max(0, overall_y_min - 0.05 * y_range)\n",
    "    overall_y_max = min(1.05, overall_y_max + 0.05 * y_range)\n",
    "    \n",
    "\n",
    "    # Set consistent y-axis limits for all subplots\n",
    "    for ax in axes.flatten():\n",
    "        ax.set_ylim(overall_y_min, overall_y_max)\n",
    "\n",
    "    # Add a single legend to the figure with increased font size\n",
    "    fig.legend(handles=legend_handles, bbox_to_anchor=(1.02, 0.5), loc='center left', fontsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    safe_reporter_name = reporter.split(\"'\")[-2].replace('.', '_')\n",
    "    plt.savefig(f\"{safe_reporter_name}_{train_size}.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take oracle from any run, as they are independently calculated\n",
    "working_dir = Path(r\"experiments\\diversify_remake\\thesis_summaries_merged\")\n",
    "combined_dir = working_dir / \"combined\"\n",
    "csv_oracle_path = Path(r\"experiments\\diversify_remake\\thesis_summaries_merged\\summary_500_contrib_merged.csv\")\n",
    "# Visualize for different reporters\n",
    "reporters = [\n",
    "    \"<class 'probes.LRProbe'>\",\n",
    "    \"<class 'probes.MMProbe'>\",\n",
    "    \"<class 'probes.CCSProbe'>\"\n",
    "]\n",
    "\n",
    "# Default 500\n",
    "train_size = 500\n",
    "transfer_types = [\"full transfer\", \"no transfer\"]\n",
    "aggregation_strategies = [\"Fixed Total\", \"Fixed Contribution\"]\n",
    "combined_csv_path = combined_dir / f\"combined_data_{train_size}.csv\"\n",
    "for reporter in reporters:\n",
    "    visualize_combined(combined_csv_path, csv_oracle_path, reporter, train_size, transfer_types, aggregation_strategies)\n",
    "\n",
    "# Default 1000\n",
    "train_size = 1000\n",
    "transfer_types = [\"full transfer\", \"no transfer\"]\n",
    "aggregation_strategies = [\"Fixed Total\"]\n",
    "combined_csv_path = combined_dir / f\"combined_data_{train_size}.csv\"\n",
    "for reporter in reporters:\n",
    "    visualize_combined(combined_csv_path, csv_oracle_path, reporter, train_size, transfer_types, aggregation_strategies)\n",
    "\n",
    "# Require Spanish\n",
    "csv_oracle_path = Path(r\"experiments\\diversify_remake\\thesis_summaries_merged\\summary_500_contrib_merged.csv\")\n",
    "train_size = 500\n",
    "transfer_types = [\"full transfer\"]\n",
    "aggregation_strategies = [\"Fixed Total\", \"Fixed Contribution\"]\n",
    "combined_csv_path = combined_dir / f\"combined_data_spanish.csv\"\n",
    "for reporter in [\"<class 'probes.LRProbe'>\"]:\n",
    "    visualize_combined(combined_csv_path, csv_oracle_path, reporter, train_size, transfer_types, aggregation_strategies)\n",
    "\n",
    "# Contrastive Inference\n",
    "reporters = [\n",
    "    (\"<class 'probes.LRProbe'>_tuple_inference\",\"<class 'probes.LRProbe'>\"),\n",
    "    (\"<class 'probes.MMProbe'>_tuple_inference\",\"<class 'probes.MMProbe'>\"),\n",
    "]\n",
    "csv_oracle_path = Path(r\"experiments\\diversify_remake\\thesis_summaries_merged\\summary_500_contrib_merged.csv\")\n",
    "train_size = 500\n",
    "transfer_types = [\"full transfer\", \"no transfer\"]\n",
    "aggregation_strategies = [\"Fixed Total\"]\n",
    "combined_csv_path = combined_dir / f\"combined_data_contrastive.csv\"\n",
    "reference_csv_path = combined_dir / f\"combined_data_{train_size}.csv\"\n",
    "for reporter, reference_reporter in reporters:\n",
    "    visualize_combined(combined_csv_path, csv_oracle_path, reporter, train_size, transfer_types, aggregation_strategies, contrastive_inference=True, reference_csv_path=reference_csv_path, reference_reporter=reference_reporter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual Illustrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set the style and color palette\n",
    "sns.set_style(\"whitegrid\")\n",
    "colors = [\"#E64B35\", \"#4DBBD5\", \"#00A087\"]\n",
    "datasets = [\"Dataset A\", \"Dataset B\", \"Dataset C\"]\n",
    "\n",
    "# Hardcoded data\n",
    "data = {\n",
    "    \"point-wise\": [\n",
    "        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],  # Dataset A\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # Dataset B\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   # Dataset C\n",
    "    ],\n",
    "    \"local\": [\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # Dataset A\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # Dataset B\n",
    "        [1, 1, 0, 1, 1, 1, 1, 1, 0, 1]   # Dataset C\n",
    "    ],\n",
    "    \"global\": [\n",
    "        [1, 1, 1, 1, 0, 1, 1, 0, 1, 0],  # Dataset A\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],  # Dataset B\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   # Dataset C\n",
    "    ]\n",
    "}\n",
    "\n",
    "def plot_frequency(ax, data, leftmost=False):\n",
    "    for i, d in enumerate(data):\n",
    "        x = np.arange(i*10 + 1, (i+1)*10 + 1)\n",
    "        ax.scatter(x, d, color=colors[i], s=20, label=datasets[i] if leftmost else \"\")\n",
    "   \n",
    "    ax.set_xlabel(\"Samples\", fontsize=\"medium\")\n",
    "    if leftmost:\n",
    "        ax.set_ylabel(\"Co-occurrence\")\n",
    "    ax.set_xlim(0, 31)\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels([])\n",
    "\n",
    "def plot_barchart(ax, data, leftmost=False):\n",
    "    correlations = [np.mean(d) for d in data]\n",
    "    x = np.arange(len(datasets))\n",
    "    ax.bar(x, correlations, color=colors)\n",
    "    if leftmost:\n",
    "        ax.set_ylabel(\"Empirical Correlation\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(datasets, fontsize=\"medium\")\n",
    "\n",
    "# Create the figure\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 3))\n",
    "\n",
    "for i, corr_type in enumerate([\"point-wise\", \"local\", \"global\"]):\n",
    "    plot_frequency(axs[0, i], data[corr_type], leftmost=(i==0))\n",
    "    plot_barchart(axs[1, i], data[corr_type], leftmost=(i==0))\n",
    "    axs[0, i].set_title(corr_type.capitalize(), fontsize=\"large\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set the Seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a new figure\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Set equal aspect ratio\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Plot axes\n",
    "ax.axhline(y=0, color='k', linewidth=0.5, zorder=0)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5, zorder=0)\n",
    "\n",
    "# Plot the blue vector\n",
    "ax.arrow(0, 0, 1, 0, color='blue', width=0.015, head_width=0.08, head_length=0.08, length_includes_head=True, zorder=1)\n",
    "\n",
    "# Plot the red vectors along the axes (dotted)\n",
    "ax.arrow(0, 0, 0.5, 0, color='red', width=0.008, head_width=0.06, head_length=0.06, length_includes_head=True, linestyle=':', zorder=2)\n",
    "ax.arrow(0, 0, 0, 0.5, color='red', width=0.008, head_width=0.06, head_length=0.06, length_includes_head=True, linestyle=':', zorder=2)\n",
    "\n",
    "# Plot the aggregated red vector (solid)\n",
    "ax.arrow(0, 0, 0.5, 0.5, color='red', width=0.015, head_width=0.08, head_length=0.08, length_includes_head=True, zorder=3)\n",
    "\n",
    "# Set the limits to start slightly before 0 and extend slightly beyond 1\n",
    "ax.set_xlim(-0.08, 1.08)\n",
    "ax.set_ylim(-0.08, 0.55)\n",
    "\n",
    "# Set ticks\n",
    "ax.set_xticks([0, 0.5, 1])\n",
    "ax.set_yticks([0, 0.5])\n",
    "\n",
    "# Add labels with larger font size\n",
    "ax.set_xlabel('Frequency of Feature 1', fontsize=14)\n",
    "ax.set_ylabel('Frequency of Feature 2', fontsize=14)\n",
    "\n",
    "# Remove top and right spines\n",
    "sns.despine()\n",
    "\n",
    "# Increase font size for all text elements\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('thm_cosine_illustration.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set the style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create data\n",
    "diversity = np.arange(1, 9)\n",
    "n_datasets = len(diversity)\n",
    "\n",
    "# Constants\n",
    "n_l = 7  # number of local features per dataset\n",
    "n_g = 5   # number of global features per dataset\n",
    "n_g_total = 20  # total number of global features\n",
    "n_pw = 100  # number of point-wise features in a dataset of size n\n",
    "n = 20  # default dataset size\n",
    "\n",
    "# Functions to generate data for each feature type and metric\n",
    "def point_wise_number(d):\n",
    "    return np.full_like(d, n_pw)\n",
    "\n",
    "def point_wise_frequency(d):\n",
    "    return np.ones_like(d) / n # For FT\n",
    "    # return n / (n * d)  # For FC, uncomment this line and comment the above line\n",
    "\n",
    "def point_wise_aggregated_salience(d):\n",
    "    return np.sqrt(point_wise_number(d) * point_wise_frequency(d)**2)\n",
    "\n",
    "def point_wise_usefulness(d):\n",
    "    return np.zeros_like(d)  # Point-wise features are not useful for predicting truth\n",
    "\n",
    "def local_number(d):\n",
    "    return n_l * d\n",
    "\n",
    "def local_frequency(d):\n",
    "    return 1 / d\n",
    "\n",
    "def local_aggregated_salience(d):\n",
    "    return np.sqrt(local_number(d) * local_frequency(d)**2)\n",
    "\n",
    "def local_usefulness(d):\n",
    "    return 1 / d  # Usefulness decreases with diversity for \"Seen\" scenario\n",
    "    # return np.zeros_like(d)  # For \"Unseen\" scenario, uncomment this line and comment the above line\n",
    "\n",
    "def global_number(d):\n",
    "    return n_g_total * (1 - np.exp(-d * n_g / n_g_total))\n",
    "\n",
    "def global_frequency(d):\n",
    "    return 1 - (1 - n_g / n_g_total) * (1 - np.exp(-(d-1)))\n",
    "\n",
    "def global_usefulness_seen(d):\n",
    "    return 1 - (1 - n_g / n_g_total) * (1 - np.exp(-(d-1)))\n",
    "\n",
    "def global_usefulness_unseen(d):\n",
    "    return np.ones_like(d) * n_g / n_g_total\n",
    "\n",
    "def global_aggregated_salience(d):\n",
    "    return np.sqrt(global_number(d) * global_frequency(d)**2)\n",
    "\n",
    "# Create the plot\n",
    "fig = plt.figure(figsize=(15, 11))\n",
    "gs = fig.add_gridspec(4, 2)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax3 = fig.add_subplot(gs[2, 0])\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "# fig.suptitle('Feature Characteristics vs Diversity', fontsize=16)\n",
    "\n",
    "# Colors for each feature type\n",
    "colors = {'Point-wise': 'blue', 'Local': 'orange', 'Global': 'green'}\n",
    "\n",
    "# Plot Number\n",
    "for feature, color in colors.items():\n",
    "    y = globals()[f\"{feature.lower().replace('-', '_')}_number\"](diversity)\n",
    "    ax1.plot(diversity, y, color=color, label=feature)\n",
    "ax1.set_ylabel('Number of Features')\n",
    "ax1.axhline(y=n_g_total, color='green', linestyle='--', alpha=0.5, label='Total Global Features')\n",
    "ax1.axhline(y=n_l, color='orange', linestyle='--', alpha=0.5, label='Local Features per Dataset')\n",
    "ax1.text(-0.1, 1.0, '(a)', transform=ax1.transAxes, fontsize=12, fontweight='bold', va='top', ha='right')\n",
    "ax1.set_yticks([0])\n",
    "ax1.set_ylim(-5, None)\n",
    "\n",
    "# Plot Frequency\n",
    "for feature, color in colors.items():\n",
    "    y = globals()[f\"{feature.lower().replace('-', '_')}_frequency\"](diversity)\n",
    "    ax2.plot(diversity, y, color=color)\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.axhline(y=n_g/n_g_total, color='green', linestyle=':', alpha=0.5, label='Global Feature Prevalence')\n",
    "ax2.text(-0.1, 1.0, '(b)', transform=ax2.transAxes, fontsize=12, fontweight='bold', va='top', ha='right')\n",
    "ax2.set_yticks([0, 1])\n",
    "ax2.set_ylim(-0.05, 1.05)  # Adjusted to show the full range\n",
    "\n",
    "# Plot Aggregated Salience\n",
    "for feature, color in colors.items():\n",
    "    y = globals()[f\"{feature.lower().replace('-', '_')}_aggregated_salience\"](diversity)\n",
    "    ax3.plot(diversity, y, color=color)\n",
    "ax3.set_ylabel('Aggregated Salience\\n(unnormalized)')\n",
    "ax3.set_xlabel('Diversity d of Training data')\n",
    "ax3.text(-0.1, 1.0, '(c)', transform=ax3.transAxes, fontsize=12, fontweight='bold', va='top', ha='right')\n",
    "ax3.set_yticks([])\n",
    "ax3.set_ylim(0, 5)\n",
    "\n",
    "# Plot Prevalence on Target (Seen)\n",
    "for feature, color in colors.items():\n",
    "    if feature == 'Point-wise':\n",
    "        y = np.zeros_like(diversity) - 0.0\n",
    "    elif feature == 'Local':\n",
    "        y = 1 / diversity\n",
    "    else:\n",
    "        y = global_usefulness_seen(diversity)\n",
    "    ax4.plot(diversity, y, color=color)\n",
    "ax4.set_ylabel('Predictive Relevance\\n(Seen)')\n",
    "ax4.axhline(y=n_g/n_g_total, color='green', linestyle=':', alpha=0.5)\n",
    "ax4.text(-0.1, 1.0, '(d)', transform=ax4.transAxes, fontsize=12, fontweight='bold', va='top', ha='right')\n",
    "ax4.set_yticks([0, 1])\n",
    "ax4.set_ylim(-0.05, 1.05)  # Adjusted to show the full range\n",
    "\n",
    "# Plot Predictive Relevance (Unseen)\n",
    "for feature, color in colors.items():\n",
    "    if feature == 'Local':\n",
    "        y = np.zeros_like(diversity) - 0.0045\n",
    "    elif feature == 'Point-wise':\n",
    "        y = np.zeros_like(diversity) + 0.0045\n",
    "    else:\n",
    "        y = global_usefulness_unseen(diversity)\n",
    "    ax5.plot(diversity, y, color=color)\n",
    "ax5.set_ylabel('Predictive Relevance\\n(Unseen)')\n",
    "ax5.set_xlabel('Diversity d of Training data')\n",
    "ax5.set_ylim(-0.1, 1)\n",
    "ax5.axhline(y=n_g/n_g_total, color='green', linestyle=':', alpha=0.5)\n",
    "ax5.text(-0.1, 1.0, '(e)', transform=ax5.transAxes, fontsize=12, fontweight='bold', va='top', ha='right')\n",
    "ax5.set_yticks([0, 1])\n",
    "ax5.set_ylim(-0.05, 1.05)  # Adjusted to show the full range\n",
    "\n",
    "# Remove x-ticks\n",
    "for ax in [ax1, ax2, ax3, ax4, ax5]:\n",
    "    ax.set_xticks([1])\n",
    "    ax.set_xticklabels([1])\n",
    "\n",
    "# Add a single legend for all subplots\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "dotted_line = plt.Line2D([0], [0], color='green', linestyle=':', label='Global Feature Prevalence')\n",
    "handles.append(dotted_line)\n",
    "labels.append('Global Feature Prevalence')\n",
    "fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(0.98, 0.98))\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
